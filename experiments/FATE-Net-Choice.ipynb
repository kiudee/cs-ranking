{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FETA-Net-Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from csrank import *\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The medoid problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrank import ChoiceDatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the medoid problem the goal of the discrete choice algorithms for the medoid problem is to find the most central object for the given set.\n",
    "This problem is inspired by solving the task of finding a good representation of the given data using the most central point of the data points\n",
    "\n",
    "We will generate a random dataset where each instance contains 30 objects and 2 features for easy plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "rootLogger = logging.getLogger('')\n",
    "rootLogger.setLevel(logging.DEBUG)\n",
    "logFormatter = logging.Formatter(\"%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\")\n",
    "consoleHandler = logging.StreamHandler()\n",
    "consoleHandler.setFormatter(logFormatter)\n",
    "rootLogger.addHandler(consoleHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-16 00:49:14,049 [MainThread  ] [INFO ]  Learning Problem: choice_function\n",
      "2019-01-16 00:49:14,071 [MainThread  ] [INFO ]  Done loading the dataset\n",
      "2019-01-16 00:49:14,072 [MainThread  ] [INFO ]  Dataset type unique\n"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "n_features = 2\n",
    "n_objects = 30\n",
    "gen = MNISTChoiceDatasetReader(dataset_type='unique', random_state=seed,\n",
    "                                n_train_instances=n_train,\n",
    "                                n_test_instances=n_test,\n",
    "                                n_objects=n_objects,\n",
    "                                n_features=n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-16 00:49:17,481 [MainThread  ] [INFO ]  Unique Dataset\n",
      "2019-01-16 00:50:15,118 [MainThread  ] [INFO ]  Done\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = gen.get_single_train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.util import get_dataset_reader, log_test_train_data, metrics_on_predictions, lp_metric_dict, \\\n",
    "    create_optimizer_parameters\n",
    "def get_results(model):\n",
    "    batch_size = X_test.shape[0]\n",
    "    s_pred = None\n",
    "    while s_pred is None:\n",
    "        try:\n",
    "            if batch_size == 0:\n",
    "                break\n",
    "            logger.info(\"Batch_size {}\".format(batch_size))\n",
    "            s_pred = model.predict_scores(X_test, batch_size=batch_size)\n",
    "        except:\n",
    "            logger.error(\"Unexpected Error {}\".format(sys.exc_info()[0]))\n",
    "            s_pred = None\n",
    "            batch_size = int(batch_size / 10)\n",
    "    y_pred = model.predict_for_scores(s_pred)\n",
    "\n",
    "    results = {'job_id': str(job_id), 'cluster_id': str(cluster_id)}\n",
    "    for name, evaluation_metric in lp_metric_dict['choice_function'].items():\n",
    "        predictions = s_pred\n",
    "        if evaluation_metric in metrics_on_predictions:\n",
    "            logger.info(\"Metric on predictions\")\n",
    "            predictions = y_pred\n",
    "        if \"NDCG\" in name:\n",
    "            evaluation_metric = make_ndcg_at_k_loss(k=n_objects)\n",
    "            predictions = y_pred\n",
    "        if isinstance(Y_test, dict):\n",
    "            metric_loss = get_mean_loss_for_dictionary(evaluation_metric, Y_test, predictions)\n",
    "        else:\n",
    "            metric_loss = get_loss_for_array(evaluation_metric, Y_test, predictions)\n",
    "        logger.info(ERROR_OUTPUT_STRING % (name, metric_loss))\n",
    "        if np.isnan(metric_loss):\n",
    "            results[name] = \"\\'Infinity\\'\"\n",
    "        else:\n",
    "            results[name] = \"{0:.4f}\".format(metric_loss)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-16 00:50:15,347 [MainThread  ] [DEBUG]  Creating the Dataset\n",
      "2019-01-16 00:50:25,428 [MainThread  ] [DEBUG]  Finished the Dataset with instances 1159704\n",
      "2019-01-16 00:50:25,430 [MainThread  ] [INFO ]  Linear SVC model \n",
      "2019-01-16 00:50:31,360 [MainThread  ] [DEBUG]  Finished Creating the model, now fitting started\n"
     ]
    }
   ],
   "source": [
    "r = RankSVMChoiceFunction(n_object_features=X_train.shape[-1])\n",
    "r.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot a random instance. The pareto points are marked as P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(d):\n",
    "    if d ==0:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return \"P\"\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "inst = np.random.choice(n_train)\n",
    "choices = np.where(Y_train[inst]==1)[0]\n",
    "ax.scatter(X_train[inst][:, 0], X_train[inst][:, 1])\n",
    "ax.scatter(X_train[inst][choices, 0], X_train[inst][choices, 1])\n",
    "for i in range(n_objects):\n",
    "    ax.text(X_train[inst, i, 0]+0.02,\n",
    "            X_train[inst, i, 1]+0.02,\n",
    "            s=get_name(int(Y_train[inst, i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FATE network\n",
    "The first-aggregate-then-evaluate approach learns an embedding of each object and then aggregates that into a _context_:\n",
    "\\begin{equation}\n",
    "\t\\mu_{C(\\vec{x})} = \\frac{1}{|C(\\vec{x})|} \\sum_{\\vec{y} \\in C(\\vec{x})} \\phi(\\vec{y})\n",
    "\\end{equation}\n",
    "and then scores each object $\\vec{x}$ using a generalized utility function $U (\\vec{x}, \\mu_{C(\\vec{x})})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fate = FATEChoiceFunction(\n",
    "    n_object_features=X_train.shape[-1],\n",
    "    optimizer=SGD(lr=1e-4, nesterov=True, momentum=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the training for only 10 epochs to get an idea of the convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fate.fit(X_train, Y_train, verbose=True, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = fate.predict_scores(X_test)\n",
    "y_pred = fate.predict_for_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csrank.metrics_np import f1_measure\n",
    "f1_measure(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not converged yet, but let us visualize the scores it assigns to test instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "inst = np.random.choice(n_test)\n",
    "choices = np.where(Y_test[inst]==1)[0]\n",
    "ax.scatter(X_test[inst][:, 0], X_test[inst][:, 1])\n",
    "ax.scatter(X_test[inst][choices, 0], X_test[inst][choices, 1])\n",
    "for i in range(n_objects):\n",
    "    if Y_test[inst, i]:\n",
    "        color = 'r'\n",
    "    else:\n",
    "        color = 'b'\n",
    "    ax.text(X_test[inst, i, 0]-0.2,\n",
    "            X_test[inst, i, 1]-0.2,\n",
    "            s='{:.1f}'.format(scores[inst][i]),\n",
    "            color=color)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
