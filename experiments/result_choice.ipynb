{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "import inspect\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "from csrank.util import setup_logging, print_dictionary\n",
    "from result_script import *\n",
    "\n",
    "from csrank.experiments import CHOICE_FUNCTIONS, CHOICE_MODELS\n",
    "from csrank.constants import CHOICE_FUNCTION\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['feta_choice',\n",
       "  'fate_choice',\n",
       "  'fetalinear_choice',\n",
       "  'fatelinear_choice',\n",
       "  'ranknet_choice',\n",
       "  'ranksvm_choice',\n",
       "  'glm_choice',\n",
       "  'random_choice'],\n",
       " ['FETA-Net',\n",
       "  'FATE-Net',\n",
       "  'FETA-Linear',\n",
       "  'FATE-Linear',\n",
       "  'RankNet',\n",
       "  'PairwiseSVM',\n",
       "  'GenLinearModel',\n",
       "  'AllPositive'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHOICE_FUNCTIONS, CHOICE_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "log_path = os.path.join(DIR_PATH, 'logs', 'results_choice.log')\n",
    "FOLDER = \"journalresults\"\n",
    "latex_path = os.path.join(DIR_PATH, FOLDER, 'choice_functions.tex')\n",
    "df_path_combined = os.path.join(DIR_PATH, FOLDER , \"ChoiceFunctions.csv\")\n",
    "\n",
    "setup_logging(log_path=log_path, level=logging.ERROR)\n",
    "logger = logging.getLogger('ResultParsing')\n",
    "datasets = ['synthetic_choice', 'mnist_choice', 'letor_choice', 'exp_choice']\n",
    "\n",
    "learning_problem = CHOICE_FUNCTION\n",
    "learning_model =  learners_map[learning_problem]\n",
    "models_dict = dict(zip(CHOICE_FUNCTIONS, CHOICE_MODELS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_val(val):\n",
    "    vals =  [float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", val)]\n",
    "    if len(vals)==1:\n",
    "        x = [vals[0], vals[0]-0.0]\n",
    "    else:\n",
    "        x = [vals[0], vals[0] - vals[1]]\n",
    "    return x\n",
    "def create_final_result(dataset, dataset_function=get_combined_results ,latex_row=False):\n",
    "    df_full = dataset_function(dataset, logger, learning_problem, latex_row=latex_row)\n",
    "    data = []\n",
    "    for dataset, df in df_full.groupby(['Dataset']):\n",
    "        for m in CHOICE_FUNCTIONS:\n",
    "            row = df[df[learning_model].str.contains(m)].values\n",
    "            onerow = None\n",
    "            if len(row) > 1:\n",
    "                if dataset_function==get_combined_results:\n",
    "                    values = np.array([get_val(val[2]) for val in row])\n",
    "                else:\n",
    "                    values = np.array([[val[2], val[2] - val[7]] for val in row])\n",
    "                maxi = np.where(values[:,0] == values[:,0][np.argmax(values[:,0])])[0][0]\n",
    "                logger.error(\"dataset {} model {}, vals {}, maxi {}\".format(dataset, row[:, 1], values, maxi))\n",
    "                row = row[maxi]\n",
    "                row[1] = models_dict[m]\n",
    "                onerow = row\n",
    "\n",
    "            elif len(row)==1:\n",
    "                row[0][1] = models_dict[m]\n",
    "                onerow = row[0]\n",
    "            if onerow is not None:\n",
    "                onerow[0] = get_dataset_name(onerow[0])\n",
    "                data.append(onerow)\n",
    "    columns = df_full.columns\n",
    "    dataframe = pd.DataFrame(data, columns=columns)\n",
    "    dataframe = dataframe.sort_values(by=[columns[0], columns[2]], ascending=[True, False])\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>ChoiceModel</th>\n",
       "      <th>$F_1$-measure</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Subset $0/1$ Accuracy</th>\n",
       "      <th>HammingAccuracy</th>\n",
       "      <th>Informedness</th>\n",
       "      <th>AUC-Score</th>\n",
       "      <th>AveragePrecisionScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.942±0.008</td>\n",
       "      <td>0.938±0.007</td>\n",
       "      <td>0.967±0.013</td>\n",
       "      <td>0.680±0.028</td>\n",
       "      <td>0.985±0.002</td>\n",
       "      <td>0.956±0.012</td>\n",
       "      <td>0.999±0.000</td>\n",
       "      <td>0.996±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.912±0.009</td>\n",
       "      <td>0.919±0.015</td>\n",
       "      <td>0.926±0.005</td>\n",
       "      <td>0.506±0.037</td>\n",
       "      <td>0.975±0.003</td>\n",
       "      <td>0.911±0.006</td>\n",
       "      <td>0.996±0.001</td>\n",
       "      <td>0.984±0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.673±0.001</td>\n",
       "      <td>0.697±0.023</td>\n",
       "      <td>0.748±0.023</td>\n",
       "      <td>0.064±0.007</td>\n",
       "      <td>0.913±0.003</td>\n",
       "      <td>0.694±0.015</td>\n",
       "      <td>0.955±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.673±0.000</td>\n",
       "      <td>0.683±0.019</td>\n",
       "      <td>0.761±0.018</td>\n",
       "      <td>0.059±0.005</td>\n",
       "      <td>0.911±0.003</td>\n",
       "      <td>0.704±0.012</td>\n",
       "      <td>0.955±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>RankNet</td>\n",
       "      <td>0.612±0.007</td>\n",
       "      <td>0.624±0.026</td>\n",
       "      <td>0.772±0.029</td>\n",
       "      <td>0.060±0.010</td>\n",
       "      <td>0.877±0.011</td>\n",
       "      <td>0.672±0.014</td>\n",
       "      <td>0.971±0.006</td>\n",
       "      <td>0.891±0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.588±0.001</td>\n",
       "      <td>0.596±0.012</td>\n",
       "      <td>0.756±0.015</td>\n",
       "      <td>0.044±0.003</td>\n",
       "      <td>0.866±0.005</td>\n",
       "      <td>0.646±0.007</td>\n",
       "      <td>0.956±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>GenLinearModel</td>\n",
       "      <td>0.585±0.008</td>\n",
       "      <td>0.604±0.022</td>\n",
       "      <td>0.738±0.023</td>\n",
       "      <td>0.044±0.005</td>\n",
       "      <td>0.869±0.009</td>\n",
       "      <td>0.633±0.013</td>\n",
       "      <td>0.952±0.007</td>\n",
       "      <td>0.861±0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.232±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dataset     ChoiceModel $F_1$-measure    Precision       Recall  \\\n",
       "0  Pareto-front        FETA-Net   0.942±0.008  0.938±0.007  0.967±0.013   \n",
       "1  Pareto-front        FATE-Net   0.912±0.009  0.919±0.015  0.926±0.005   \n",
       "2  Pareto-front     FETA-Linear   0.673±0.001  0.697±0.023  0.748±0.023   \n",
       "3  Pareto-front     FATE-Linear   0.673±0.000  0.683±0.019  0.761±0.018   \n",
       "4  Pareto-front         RankNet   0.612±0.007  0.624±0.026  0.772±0.029   \n",
       "5  Pareto-front     PairwiseSVM   0.588±0.001  0.596±0.012  0.756±0.015   \n",
       "6  Pareto-front  GenLinearModel   0.585±0.008  0.604±0.022  0.738±0.023   \n",
       "7  Pareto-front     AllPositive   0.232±0.000  0.133±0.000  1.000±0.000   \n",
       "\n",
       "  Subset $0/1$ Accuracy HammingAccuracy Informedness    AUC-Score  \\\n",
       "0           0.680±0.028     0.985±0.002  0.956±0.012  0.999±0.000   \n",
       "1           0.506±0.037     0.975±0.003  0.911±0.006  0.996±0.001   \n",
       "2           0.064±0.007     0.913±0.003  0.694±0.015  0.955±0.000   \n",
       "3           0.059±0.005     0.911±0.003  0.704±0.012  0.955±0.000   \n",
       "4           0.060±0.010     0.877±0.011  0.672±0.014  0.971±0.006   \n",
       "5           0.044±0.003     0.866±0.005  0.646±0.007  0.956±0.000   \n",
       "6           0.044±0.005     0.869±0.009  0.633±0.013  0.952±0.007   \n",
       "7           0.000±0.000     0.133±0.000  0.000±0.000  0.500±0.000   \n",
       "\n",
       "  AveragePrecisionScore  \n",
       "0           0.996±0.000  \n",
       "1           0.984±0.003  \n",
       "2           0.865±0.000  \n",
       "3           0.865±0.000  \n",
       "4           0.891±0.019  \n",
       "5           0.865±0.000  \n",
       "6           0.861±0.011  \n",
       "7           0.133±0.000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = datasets[0]\n",
    "df = create_final_result(d, latex_row=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Pareto-front', 'MNIST\\nMode', 'MNIST\\nUnique', 'LETOR\\nMQ2007',\n",
       "        'LETOR\\nMQ2008', 'Expedia'], dtype=object),\n",
       " array(['FETA-Net', 'FATE-Net', 'RankNet', 'PairwiseSVM', 'GenLinearModel',\n",
       "        'AllPositive'], dtype=object))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from csrank.experiments.constants import FETALINEAR_CHOICE, FATELINEAR_CHOICE\n",
    "dataFrame = None\n",
    "for dataset in datasets:\n",
    "    df = create_final_result(dataset, get_combined_results_plot ,latex_row=False)\n",
    "    if dataFrame is None:\n",
    "        dataFrame = copy.copy(df)\n",
    "    else:\n",
    "        dataFrame = dataFrame.append(df, ignore_index=True)\n",
    "\n",
    "searchFor = [\"5 Objects\", \"Critique\"]\n",
    "df = dataFrame[~dataFrame.Dataset.str.contains('|'.join(searchFor))]\n",
    "df.replace(to_replace=r' 10 Objects', value='', regex=True, inplace=True)\n",
    "df.replace(to_replace=r'LETOR-', value='LETOR\\n', regex=True, inplace=True)\n",
    "df.replace(to_replace=r'MNIST-', value='MNIST\\n', regex=True, inplace=True)\n",
    "searchFor = [models_dict[FETALINEAR_CHOICE], models_dict[FATELINEAR_CHOICE]]\n",
    "df = df[~df[learning_model].str.contains('|'.join(searchFor))]\n",
    "df.Dataset.unique(), df[learning_model].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 563x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from csrank.constants import DISCRETE_CHOICE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "plt.style.use('default')\n",
    "# create plot\n",
    "extension = 'pdf'\n",
    "fname = os.path.join(DIR_PATH, FOLDER, \"{}_{}.{}\")\n",
    "start = 0.0\n",
    "ncols = 3\n",
    "params = dict(loc='lower right', bbox_to_anchor=(0.8, -0.4), ncol=ncols, fancybox=False, shadow=True,\n",
    "                facecolor='white', edgecolor='k', fontsize=7)\n",
    "plts = bar_plot_for_problem2(df, learning_problem, start, params, extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>ChoiceModel</th>\n",
       "      <th>$F_1$Measure</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Subset $0/1$ Accuracy</th>\n",
       "      <th>HammingAccuracy</th>\n",
       "      <th>Informedness</th>\n",
       "      <th>AUC-Score</th>\n",
       "      <th>AveragePrecisionScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.942±0.008</td>\n",
       "      <td>0.938±0.007</td>\n",
       "      <td>0.967±0.013</td>\n",
       "      <td>0.680±0.028</td>\n",
       "      <td>0.985±0.002</td>\n",
       "      <td>0.956±0.012</td>\n",
       "      <td>0.999±0.000</td>\n",
       "      <td>0.996±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.912±0.009</td>\n",
       "      <td>0.919±0.015</td>\n",
       "      <td>0.926±0.005</td>\n",
       "      <td>0.506±0.037</td>\n",
       "      <td>0.975±0.003</td>\n",
       "      <td>0.911±0.006</td>\n",
       "      <td>0.996±0.001</td>\n",
       "      <td>0.984±0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.673±0.001</td>\n",
       "      <td>0.697±0.023</td>\n",
       "      <td>0.748±0.023</td>\n",
       "      <td>0.064±0.007</td>\n",
       "      <td>0.913±0.003</td>\n",
       "      <td>0.694±0.015</td>\n",
       "      <td>0.955±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.673±0.000</td>\n",
       "      <td>0.683±0.019</td>\n",
       "      <td>0.761±0.018</td>\n",
       "      <td>0.059±0.005</td>\n",
       "      <td>0.911±0.003</td>\n",
       "      <td>0.704±0.012</td>\n",
       "      <td>0.955±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.612±0.007</td>\n",
       "      <td>0.624±0.026</td>\n",
       "      <td>0.772±0.029</td>\n",
       "      <td>0.060±0.010</td>\n",
       "      <td>0.877±0.011</td>\n",
       "      <td>0.672±0.014</td>\n",
       "      <td>0.971±0.006</td>\n",
       "      <td>0.891±0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.588±0.001</td>\n",
       "      <td>0.596±0.012</td>\n",
       "      <td>0.756±0.015</td>\n",
       "      <td>0.044±0.003</td>\n",
       "      <td>0.866±0.005</td>\n",
       "      <td>0.646±0.007</td>\n",
       "      <td>0.956±0.000</td>\n",
       "      <td>0.865±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.585±0.008</td>\n",
       "      <td>0.604±0.022</td>\n",
       "      <td>0.738±0.023</td>\n",
       "      <td>0.044±0.005</td>\n",
       "      <td>0.869±0.009</td>\n",
       "      <td>0.633±0.013</td>\n",
       "      <td>0.952±0.007</td>\n",
       "      <td>0.861±0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pareto-front</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.232±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.133±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mode</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.976±0.001</td>\n",
       "      <td>0.980±0.003</td>\n",
       "      <td>0.979±0.004</td>\n",
       "      <td>0.883±0.011</td>\n",
       "      <td>0.978±0.001</td>\n",
       "      <td>0.961±0.002</td>\n",
       "      <td>0.992±0.001</td>\n",
       "      <td>0.991±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mode</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.809±0.005</td>\n",
       "      <td>0.742±0.003</td>\n",
       "      <td>0.962±0.009</td>\n",
       "      <td>0.311±0.032</td>\n",
       "      <td>0.809±0.004</td>\n",
       "      <td>0.695±0.009</td>\n",
       "      <td>0.981±0.006</td>\n",
       "      <td>0.980±0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mode</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.597±0.001</td>\n",
       "      <td>0.443±0.001</td>\n",
       "      <td>0.996±0.004</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.445±0.001</td>\n",
       "      <td>0.003±0.002</td>\n",
       "      <td>0.516±0.001</td>\n",
       "      <td>0.573±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mode</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.597±0.001</td>\n",
       "      <td>0.444±0.002</td>\n",
       "      <td>0.992±0.005</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.447±0.004</td>\n",
       "      <td>0.007±0.006</td>\n",
       "      <td>0.517±0.002</td>\n",
       "      <td>0.573±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mode</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.597±0.001</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.999±0.002</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.509±0.006</td>\n",
       "      <td>0.569±0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Mode</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.597±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.503±0.002</td>\n",
       "      <td>0.563±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Mode</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.597±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.999±0.001</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.497±0.004</td>\n",
       "      <td>0.561±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mode</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.597±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.003±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.443±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Unique</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.973±0.004</td>\n",
       "      <td>0.975±0.002</td>\n",
       "      <td>0.977±0.007</td>\n",
       "      <td>0.848±0.021</td>\n",
       "      <td>0.980±0.003</td>\n",
       "      <td>0.960±0.006</td>\n",
       "      <td>0.995±0.001</td>\n",
       "      <td>0.992±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Unique</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.963±0.003</td>\n",
       "      <td>0.963±0.006</td>\n",
       "      <td>0.975±0.004</td>\n",
       "      <td>0.814±0.020</td>\n",
       "      <td>0.972±0.003</td>\n",
       "      <td>0.945±0.005</td>\n",
       "      <td>0.992±0.001</td>\n",
       "      <td>0.989±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Unique</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.562±0.001</td>\n",
       "      <td>0.404±0.001</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.001</td>\n",
       "      <td>0.405±0.001</td>\n",
       "      <td>0.000±0.001</td>\n",
       "      <td>0.517±0.001</td>\n",
       "      <td>0.557±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Unique</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.562±0.001</td>\n",
       "      <td>0.405±0.001</td>\n",
       "      <td>0.999±0.002</td>\n",
       "      <td>0.001±0.000</td>\n",
       "      <td>0.406±0.002</td>\n",
       "      <td>0.001±0.003</td>\n",
       "      <td>0.505±0.007</td>\n",
       "      <td>0.560±0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Unique</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.562±0.001</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>0.999±0.002</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.405±0.001</td>\n",
       "      <td>0.000±0.001</td>\n",
       "      <td>0.511±0.006</td>\n",
       "      <td>0.553±0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Unique</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.562±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>1.000±0.001</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.504±0.001</td>\n",
       "      <td>0.538±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Unique</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.562±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>1.000±0.001</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.508±0.004</td>\n",
       "      <td>0.542±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Unique</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.562±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.405±0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.477±0.004</td>\n",
       "      <td>0.413±0.007</td>\n",
       "      <td>0.914±0.007</td>\n",
       "      <td>0.007±0.002</td>\n",
       "      <td>0.506±0.002</td>\n",
       "      <td>0.235±0.009</td>\n",
       "      <td>0.729±0.011</td>\n",
       "      <td>0.572±0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.470±0.002</td>\n",
       "      <td>0.405±0.001</td>\n",
       "      <td>0.913±0.002</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.501±0.003</td>\n",
       "      <td>0.232±0.002</td>\n",
       "      <td>0.704±0.002</td>\n",
       "      <td>0.540±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.453±0.021</td>\n",
       "      <td>0.367±0.022</td>\n",
       "      <td>0.854±0.029</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.514±0.031</td>\n",
       "      <td>0.220±0.026</td>\n",
       "      <td>0.696±0.007</td>\n",
       "      <td>0.532±0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.452±0.022</td>\n",
       "      <td>0.372±0.036</td>\n",
       "      <td>0.837±0.049</td>\n",
       "      <td>0.001±0.002</td>\n",
       "      <td>0.526±0.049</td>\n",
       "      <td>0.231±0.035</td>\n",
       "      <td>0.694±0.005</td>\n",
       "      <td>0.540±0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.452±0.021</td>\n",
       "      <td>0.362±0.025</td>\n",
       "      <td>0.865±0.044</td>\n",
       "      <td>0.001±0.002</td>\n",
       "      <td>0.504±0.032</td>\n",
       "      <td>0.212±0.021</td>\n",
       "      <td>0.695±0.006</td>\n",
       "      <td>0.540±0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LETOR MQ2007 10 Objects</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.427±0.021</td>\n",
       "      <td>0.317±0.022</td>\n",
       "      <td>0.965±0.037</td>\n",
       "      <td>0.001±0.002</td>\n",
       "      <td>0.358±0.039</td>\n",
       "      <td>0.058±0.029</td>\n",
       "      <td>0.614±0.009</td>\n",
       "      <td>0.465±0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.452±0.021</td>\n",
       "      <td>0.355±0.022</td>\n",
       "      <td>0.886±0.024</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.489±0.024</td>\n",
       "      <td>0.207±0.024</td>\n",
       "      <td>0.694±0.005</td>\n",
       "      <td>0.539±0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.450±0.019</td>\n",
       "      <td>0.352±0.018</td>\n",
       "      <td>0.892±0.044</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.480±0.032</td>\n",
       "      <td>0.196±0.028</td>\n",
       "      <td>0.695±0.004</td>\n",
       "      <td>0.539±0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.444±0.022</td>\n",
       "      <td>0.344±0.029</td>\n",
       "      <td>0.917±0.031</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.444±0.043</td>\n",
       "      <td>0.161±0.028</td>\n",
       "      <td>0.699±0.004</td>\n",
       "      <td>0.540±0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.436±0.008</td>\n",
       "      <td>0.318±0.005</td>\n",
       "      <td>1.002±0.012</td>\n",
       "      <td>-0.008±0.012</td>\n",
       "      <td>0.329±0.014</td>\n",
       "      <td>0.032±0.013</td>\n",
       "      <td>0.612±0.012</td>\n",
       "      <td>0.464±0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.427±0.022</td>\n",
       "      <td>0.316±0.023</td>\n",
       "      <td>0.973±0.018</td>\n",
       "      <td>0.001±0.002</td>\n",
       "      <td>0.350±0.035</td>\n",
       "      <td>0.051±0.019</td>\n",
       "      <td>0.613±0.012</td>\n",
       "      <td>0.465±0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>LETOR MQ2007 5 Objects</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.421±0.021</td>\n",
       "      <td>0.306±0.020</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.001±0.002</td>\n",
       "      <td>0.306±0.020</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.306±0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.540±0.005</td>\n",
       "      <td>0.499±0.010</td>\n",
       "      <td>0.795±0.006</td>\n",
       "      <td>0.041±0.002</td>\n",
       "      <td>0.676±0.002</td>\n",
       "      <td>0.431±0.002</td>\n",
       "      <td>0.837±0.006</td>\n",
       "      <td>0.685±0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.537±0.001</td>\n",
       "      <td>0.495±0.002</td>\n",
       "      <td>0.801±0.002</td>\n",
       "      <td>0.045±0.002</td>\n",
       "      <td>0.693±0.002</td>\n",
       "      <td>0.440±0.003</td>\n",
       "      <td>0.842±0.003</td>\n",
       "      <td>0.684±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.530±0.009</td>\n",
       "      <td>0.486±0.012</td>\n",
       "      <td>0.778±0.012</td>\n",
       "      <td>0.038±0.009</td>\n",
       "      <td>0.681±0.006</td>\n",
       "      <td>0.433±0.008</td>\n",
       "      <td>0.830±0.005</td>\n",
       "      <td>0.678±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.529±0.005</td>\n",
       "      <td>0.497±0.005</td>\n",
       "      <td>0.791±0.010</td>\n",
       "      <td>0.026±0.009</td>\n",
       "      <td>0.692±0.005</td>\n",
       "      <td>0.421±0.012</td>\n",
       "      <td>0.803±0.009</td>\n",
       "      <td>0.679±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.526±0.022</td>\n",
       "      <td>0.446±0.029</td>\n",
       "      <td>0.846±0.041</td>\n",
       "      <td>0.042±0.022</td>\n",
       "      <td>0.644±0.025</td>\n",
       "      <td>0.428±0.015</td>\n",
       "      <td>0.786±0.018</td>\n",
       "      <td>0.655±0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.493±0.028</td>\n",
       "      <td>0.387±0.038</td>\n",
       "      <td>0.901±0.069</td>\n",
       "      <td>0.014±0.010</td>\n",
       "      <td>0.545±0.062</td>\n",
       "      <td>0.311±0.061</td>\n",
       "      <td>0.739±0.019</td>\n",
       "      <td>0.598±0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.461±0.002</td>\n",
       "      <td>0.474±0.003</td>\n",
       "      <td>0.609±0.003</td>\n",
       "      <td>0.017±0.004</td>\n",
       "      <td>0.658±0.003</td>\n",
       "      <td>0.323±0.002</td>\n",
       "      <td>0.758±0.004</td>\n",
       "      <td>0.594±0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>LETOR MQ2008 10 Objects</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.424±0.021</td>\n",
       "      <td>0.298±0.020</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.298±0.020</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.298±0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.546±0.006</td>\n",
       "      <td>0.421±0.010</td>\n",
       "      <td>0.907±0.006</td>\n",
       "      <td>0.049±0.008</td>\n",
       "      <td>0.616±0.009</td>\n",
       "      <td>0.430±0.009</td>\n",
       "      <td>0.814±0.008</td>\n",
       "      <td>0.675±0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.539±0.011</td>\n",
       "      <td>0.420±0.009</td>\n",
       "      <td>0.900±0.006</td>\n",
       "      <td>0.050±0.011</td>\n",
       "      <td>0.617±0.010</td>\n",
       "      <td>0.425±0.003</td>\n",
       "      <td>0.825±0.007</td>\n",
       "      <td>0.676±0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.533±0.005</td>\n",
       "      <td>0.436±0.021</td>\n",
       "      <td>0.873±0.030</td>\n",
       "      <td>0.031±0.023</td>\n",
       "      <td>0.636±0.015</td>\n",
       "      <td>0.433±0.024</td>\n",
       "      <td>0.808±0.027</td>\n",
       "      <td>0.665±0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.526±0.010</td>\n",
       "      <td>0.435±0.032</td>\n",
       "      <td>0.883±0.048</td>\n",
       "      <td>0.056±0.007</td>\n",
       "      <td>0.634±0.016</td>\n",
       "      <td>0.438±0.006</td>\n",
       "      <td>0.802±0.015</td>\n",
       "      <td>0.653±0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.524±0.023</td>\n",
       "      <td>0.438±0.039</td>\n",
       "      <td>0.866±0.045</td>\n",
       "      <td>0.037±0.013</td>\n",
       "      <td>0.627±0.034</td>\n",
       "      <td>0.418±0.025</td>\n",
       "      <td>0.794±0.013</td>\n",
       "      <td>0.662±0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.497±0.029</td>\n",
       "      <td>0.392±0.034</td>\n",
       "      <td>0.892±0.025</td>\n",
       "      <td>0.021±0.024</td>\n",
       "      <td>0.567±0.038</td>\n",
       "      <td>0.337±0.059</td>\n",
       "      <td>0.742±0.038</td>\n",
       "      <td>0.606±0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.462±0.002</td>\n",
       "      <td>0.471±0.003</td>\n",
       "      <td>0.607±0.004</td>\n",
       "      <td>0.016±0.002</td>\n",
       "      <td>0.662±0.002</td>\n",
       "      <td>0.324±0.002</td>\n",
       "      <td>0.758±0.003</td>\n",
       "      <td>0.596±0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>LETOR MQ2008 5 Objects</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.424±0.021</td>\n",
       "      <td>0.298±0.020</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.298±0.020</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.298±0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>FATE-Net</td>\n",
       "      <td>0.198±0.006</td>\n",
       "      <td>0.133±0.005</td>\n",
       "      <td>0.546±0.015</td>\n",
       "      <td>0.018±0.002</td>\n",
       "      <td>0.782±0.010</td>\n",
       "      <td>0.346±0.010</td>\n",
       "      <td>0.707±0.007</td>\n",
       "      <td>0.378±0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>FETA-Net</td>\n",
       "      <td>0.186±0.001</td>\n",
       "      <td>0.119±0.002</td>\n",
       "      <td>0.590±0.018</td>\n",
       "      <td>0.009±0.002</td>\n",
       "      <td>0.727±0.014</td>\n",
       "      <td>0.322±0.003</td>\n",
       "      <td>0.688±0.001</td>\n",
       "      <td>0.354±0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>FETA-Linear</td>\n",
       "      <td>0.179±0.007</td>\n",
       "      <td>0.121±0.006</td>\n",
       "      <td>0.539±0.011</td>\n",
       "      <td>0.020±0.002</td>\n",
       "      <td>0.765±0.015</td>\n",
       "      <td>0.324±0.006</td>\n",
       "      <td>0.696±0.007</td>\n",
       "      <td>0.367±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>FATE-Linear</td>\n",
       "      <td>0.168±0.005</td>\n",
       "      <td>0.114±0.009</td>\n",
       "      <td>0.510±0.011</td>\n",
       "      <td>0.018±0.009</td>\n",
       "      <td>0.769±0.011</td>\n",
       "      <td>0.306±0.010</td>\n",
       "      <td>0.695±0.012</td>\n",
       "      <td>0.354±0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>RankNet-Choice</td>\n",
       "      <td>0.167±0.017</td>\n",
       "      <td>0.101±0.012</td>\n",
       "      <td>0.638±0.046</td>\n",
       "      <td>0.003±0.001</td>\n",
       "      <td>0.650±0.062</td>\n",
       "      <td>0.278±0.034</td>\n",
       "      <td>0.716±0.006</td>\n",
       "      <td>0.363±0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>PairwiseSVM</td>\n",
       "      <td>0.129±0.017</td>\n",
       "      <td>0.077±0.013</td>\n",
       "      <td>0.703±0.149</td>\n",
       "      <td>0.004±0.002</td>\n",
       "      <td>0.481±0.227</td>\n",
       "      <td>0.165±0.097</td>\n",
       "      <td>0.680±0.051</td>\n",
       "      <td>0.321±0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>GeneralizedLinearModel</td>\n",
       "      <td>0.107±0.001</td>\n",
       "      <td>0.059±0.001</td>\n",
       "      <td>0.992±0.013</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.069±0.018</td>\n",
       "      <td>0.004±0.007</td>\n",
       "      <td>0.503±0.102</td>\n",
       "      <td>0.192±0.050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Expedia 10 Objects</td>\n",
       "      <td>AllPositive</td>\n",
       "      <td>0.106±0.000</td>\n",
       "      <td>0.058±0.000</td>\n",
       "      <td>1.000±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.058±0.000</td>\n",
       "      <td>0.000±0.000</td>\n",
       "      <td>0.500±0.000</td>\n",
       "      <td>0.058±0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Dataset             ChoiceModel $F_1$Measure    Precision  \\\n",
       "0              Pareto-front                FETA-Net  0.942±0.008  0.938±0.007   \n",
       "1              Pareto-front                FATE-Net  0.912±0.009  0.919±0.015   \n",
       "2              Pareto-front             FETA-Linear  0.673±0.001  0.697±0.023   \n",
       "3              Pareto-front             FATE-Linear  0.673±0.000  0.683±0.019   \n",
       "4              Pareto-front          RankNet-Choice  0.612±0.007  0.624±0.026   \n",
       "5              Pareto-front             PairwiseSVM  0.588±0.001  0.596±0.012   \n",
       "6              Pareto-front  GeneralizedLinearModel  0.585±0.008  0.604±0.022   \n",
       "7              Pareto-front             AllPositive  0.232±0.000  0.133±0.000   \n",
       "8                      Mode                FATE-Net  0.976±0.001  0.980±0.003   \n",
       "9                      Mode                FETA-Net  0.809±0.005  0.742±0.003   \n",
       "10                     Mode             FETA-Linear  0.597±0.001  0.443±0.001   \n",
       "11                     Mode             FATE-Linear  0.597±0.001  0.444±0.002   \n",
       "12                     Mode             PairwiseSVM  0.597±0.001  0.443±0.000   \n",
       "13                     Mode          RankNet-Choice  0.597±0.000  0.443±0.000   \n",
       "14                     Mode  GeneralizedLinearModel  0.597±0.000  0.443±0.000   \n",
       "15                     Mode             AllPositive  0.597±0.000  0.443±0.000   \n",
       "16                   Unique                FATE-Net  0.973±0.004  0.975±0.002   \n",
       "17                   Unique                FETA-Net  0.963±0.003  0.963±0.006   \n",
       "18                   Unique             FETA-Linear  0.562±0.001  0.404±0.001   \n",
       "19                   Unique             FATE-Linear  0.562±0.001  0.405±0.001   \n",
       "20                   Unique             PairwiseSVM  0.562±0.001  0.405±0.000   \n",
       "21                   Unique          RankNet-Choice  0.562±0.000  0.405±0.000   \n",
       "22                   Unique  GeneralizedLinearModel  0.562±0.000  0.405±0.000   \n",
       "23                   Unique             AllPositive  0.562±0.000  0.405±0.000   \n",
       "24  LETOR MQ2007 10 Objects                FETA-Net  0.477±0.004  0.413±0.007   \n",
       "25  LETOR MQ2007 10 Objects                FATE-Net  0.470±0.002  0.405±0.001   \n",
       "26  LETOR MQ2007 10 Objects             PairwiseSVM  0.453±0.021  0.367±0.022   \n",
       "27  LETOR MQ2007 10 Objects             FETA-Linear  0.452±0.022  0.372±0.036   \n",
       "28  LETOR MQ2007 10 Objects             FATE-Linear  0.452±0.021  0.362±0.025   \n",
       "29  LETOR MQ2007 10 Objects  GeneralizedLinearModel  0.427±0.021  0.317±0.022   \n",
       "..                      ...                     ...          ...          ...   \n",
       "34   LETOR MQ2007 5 Objects             FATE-Linear  0.452±0.021  0.355±0.022   \n",
       "35   LETOR MQ2007 5 Objects             FETA-Linear  0.450±0.019  0.352±0.018   \n",
       "36   LETOR MQ2007 5 Objects             PairwiseSVM  0.444±0.022  0.344±0.029   \n",
       "37   LETOR MQ2007 5 Objects          RankNet-Choice  0.436±0.008  0.318±0.005   \n",
       "38   LETOR MQ2007 5 Objects  GeneralizedLinearModel  0.427±0.022  0.316±0.023   \n",
       "39   LETOR MQ2007 5 Objects             AllPositive  0.421±0.021  0.306±0.020   \n",
       "40  LETOR MQ2008 10 Objects                FATE-Net  0.540±0.005  0.499±0.010   \n",
       "41  LETOR MQ2008 10 Objects                FETA-Net  0.537±0.001  0.495±0.002   \n",
       "42  LETOR MQ2008 10 Objects             FATE-Linear  0.530±0.009  0.486±0.012   \n",
       "43  LETOR MQ2008 10 Objects             FETA-Linear  0.529±0.005  0.497±0.005   \n",
       "44  LETOR MQ2008 10 Objects             PairwiseSVM  0.526±0.022  0.446±0.029   \n",
       "45  LETOR MQ2008 10 Objects  GeneralizedLinearModel  0.493±0.028  0.387±0.038   \n",
       "46  LETOR MQ2008 10 Objects          RankNet-Choice  0.461±0.002  0.474±0.003   \n",
       "47  LETOR MQ2008 10 Objects             AllPositive  0.424±0.021  0.298±0.020   \n",
       "48   LETOR MQ2008 5 Objects                FATE-Net  0.546±0.006  0.421±0.010   \n",
       "49   LETOR MQ2008 5 Objects                FETA-Net  0.539±0.011  0.420±0.009   \n",
       "50   LETOR MQ2008 5 Objects             FATE-Linear  0.533±0.005  0.436±0.021   \n",
       "51   LETOR MQ2008 5 Objects             FETA-Linear  0.526±0.010  0.435±0.032   \n",
       "52   LETOR MQ2008 5 Objects             PairwiseSVM  0.524±0.023  0.438±0.039   \n",
       "53   LETOR MQ2008 5 Objects  GeneralizedLinearModel  0.497±0.029  0.392±0.034   \n",
       "54   LETOR MQ2008 5 Objects          RankNet-Choice  0.462±0.002  0.471±0.003   \n",
       "55   LETOR MQ2008 5 Objects             AllPositive  0.424±0.021  0.298±0.020   \n",
       "56       Expedia 10 Objects                FATE-Net  0.198±0.006  0.133±0.005   \n",
       "57       Expedia 10 Objects                FETA-Net  0.186±0.001  0.119±0.002   \n",
       "58       Expedia 10 Objects             FETA-Linear  0.179±0.007  0.121±0.006   \n",
       "59       Expedia 10 Objects             FATE-Linear  0.168±0.005  0.114±0.009   \n",
       "60       Expedia 10 Objects          RankNet-Choice  0.167±0.017  0.101±0.012   \n",
       "61       Expedia 10 Objects             PairwiseSVM  0.129±0.017  0.077±0.013   \n",
       "62       Expedia 10 Objects  GeneralizedLinearModel  0.107±0.001  0.059±0.001   \n",
       "63       Expedia 10 Objects             AllPositive  0.106±0.000  0.058±0.000   \n",
       "\n",
       "         Recall Subset $0/1$ Accuracy HammingAccuracy Informedness  \\\n",
       "0   0.967±0.013           0.680±0.028     0.985±0.002  0.956±0.012   \n",
       "1   0.926±0.005           0.506±0.037     0.975±0.003  0.911±0.006   \n",
       "2   0.748±0.023           0.064±0.007     0.913±0.003  0.694±0.015   \n",
       "3   0.761±0.018           0.059±0.005     0.911±0.003  0.704±0.012   \n",
       "4   0.772±0.029           0.060±0.010     0.877±0.011  0.672±0.014   \n",
       "5   0.756±0.015           0.044±0.003     0.866±0.005  0.646±0.007   \n",
       "6   0.738±0.023           0.044±0.005     0.869±0.009  0.633±0.013   \n",
       "7   1.000±0.000           0.000±0.000     0.133±0.000  0.000±0.000   \n",
       "8   0.979±0.004           0.883±0.011     0.978±0.001  0.961±0.002   \n",
       "9   0.962±0.009           0.311±0.032     0.809±0.004  0.695±0.009   \n",
       "10  0.996±0.004           0.003±0.000     0.445±0.001  0.003±0.002   \n",
       "11  0.992±0.005           0.003±0.000     0.447±0.004  0.007±0.006   \n",
       "12  0.999±0.002           0.003±0.000     0.443±0.000  0.000±0.000   \n",
       "13  1.000±0.000           0.003±0.000     0.443±0.000  0.000±0.000   \n",
       "14  0.999±0.001           0.003±0.000     0.443±0.000  0.000±0.000   \n",
       "15  1.000±0.000           0.003±0.000     0.443±0.000  0.000±0.000   \n",
       "16  0.977±0.007           0.848±0.021     0.980±0.003  0.960±0.006   \n",
       "17  0.975±0.004           0.814±0.020     0.972±0.003  0.945±0.005   \n",
       "18  1.000±0.000           0.000±0.001     0.405±0.001  0.000±0.001   \n",
       "19  0.999±0.002           0.001±0.000     0.406±0.002  0.001±0.003   \n",
       "20  0.999±0.002           0.000±0.000     0.405±0.001  0.000±0.001   \n",
       "21  1.000±0.001           0.000±0.000     0.405±0.000  0.000±0.000   \n",
       "22  1.000±0.001           0.000±0.000     0.405±0.000  0.000±0.000   \n",
       "23  1.000±0.000           0.000±0.000     0.405±0.000  0.000±0.000   \n",
       "24  0.914±0.007           0.007±0.002     0.506±0.002  0.235±0.009   \n",
       "25  0.913±0.002           0.000±0.000     0.501±0.003  0.232±0.002   \n",
       "26  0.854±0.029           0.000±0.000     0.514±0.031  0.220±0.026   \n",
       "27  0.837±0.049           0.001±0.002     0.526±0.049  0.231±0.035   \n",
       "28  0.865±0.044           0.001±0.002     0.504±0.032  0.212±0.021   \n",
       "29  0.965±0.037           0.001±0.002     0.358±0.039  0.058±0.029   \n",
       "..          ...                   ...             ...          ...   \n",
       "34  0.886±0.024           0.000±0.000     0.489±0.024  0.207±0.024   \n",
       "35  0.892±0.044           0.000±0.000     0.480±0.032  0.196±0.028   \n",
       "36  0.917±0.031           0.000±0.000     0.444±0.043  0.161±0.028   \n",
       "37  1.002±0.012          -0.008±0.012     0.329±0.014  0.032±0.013   \n",
       "38  0.973±0.018           0.001±0.002     0.350±0.035  0.051±0.019   \n",
       "39  1.000±0.000           0.001±0.002     0.306±0.020  0.000±0.000   \n",
       "40  0.795±0.006           0.041±0.002     0.676±0.002  0.431±0.002   \n",
       "41  0.801±0.002           0.045±0.002     0.693±0.002  0.440±0.003   \n",
       "42  0.778±0.012           0.038±0.009     0.681±0.006  0.433±0.008   \n",
       "43  0.791±0.010           0.026±0.009     0.692±0.005  0.421±0.012   \n",
       "44  0.846±0.041           0.042±0.022     0.644±0.025  0.428±0.015   \n",
       "45  0.901±0.069           0.014±0.010     0.545±0.062  0.311±0.061   \n",
       "46  0.609±0.003           0.017±0.004     0.658±0.003  0.323±0.002   \n",
       "47  1.000±0.000           0.000±0.000     0.298±0.020  0.000±0.000   \n",
       "48  0.907±0.006           0.049±0.008     0.616±0.009  0.430±0.009   \n",
       "49  0.900±0.006           0.050±0.011     0.617±0.010  0.425±0.003   \n",
       "50  0.873±0.030           0.031±0.023     0.636±0.015  0.433±0.024   \n",
       "51  0.883±0.048           0.056±0.007     0.634±0.016  0.438±0.006   \n",
       "52  0.866±0.045           0.037±0.013     0.627±0.034  0.418±0.025   \n",
       "53  0.892±0.025           0.021±0.024     0.567±0.038  0.337±0.059   \n",
       "54  0.607±0.004           0.016±0.002     0.662±0.002  0.324±0.002   \n",
       "55  1.000±0.000           0.000±0.000     0.298±0.020  0.000±0.000   \n",
       "56  0.546±0.015           0.018±0.002     0.782±0.010  0.346±0.010   \n",
       "57  0.590±0.018           0.009±0.002     0.727±0.014  0.322±0.003   \n",
       "58  0.539±0.011           0.020±0.002     0.765±0.015  0.324±0.006   \n",
       "59  0.510±0.011           0.018±0.009     0.769±0.011  0.306±0.010   \n",
       "60  0.638±0.046           0.003±0.001     0.650±0.062  0.278±0.034   \n",
       "61  0.703±0.149           0.004±0.002     0.481±0.227  0.165±0.097   \n",
       "62  0.992±0.013           0.000±0.000     0.069±0.018  0.004±0.007   \n",
       "63  1.000±0.000           0.000±0.000     0.058±0.000  0.000±0.000   \n",
       "\n",
       "      AUC-Score AveragePrecisionScore  \n",
       "0   0.999±0.000           0.996±0.000  \n",
       "1   0.996±0.001           0.984±0.003  \n",
       "2   0.955±0.000           0.865±0.000  \n",
       "3   0.955±0.000           0.865±0.000  \n",
       "4   0.971±0.006           0.891±0.019  \n",
       "5   0.956±0.000           0.865±0.000  \n",
       "6   0.952±0.007           0.861±0.011  \n",
       "7   0.500±0.000           0.133±0.000  \n",
       "8   0.992±0.001           0.991±0.002  \n",
       "9   0.981±0.006           0.980±0.006  \n",
       "10  0.516±0.001           0.573±0.001  \n",
       "11  0.517±0.002           0.573±0.002  \n",
       "12  0.509±0.006           0.569±0.004  \n",
       "13  0.503±0.002           0.563±0.002  \n",
       "14  0.497±0.004           0.561±0.002  \n",
       "15  0.500±0.000           0.443±0.000  \n",
       "16  0.995±0.001           0.992±0.001  \n",
       "17  0.992±0.001           0.989±0.001  \n",
       "18  0.517±0.001           0.557±0.001  \n",
       "19  0.505±0.007           0.560±0.007  \n",
       "20  0.511±0.006           0.553±0.005  \n",
       "21  0.504±0.001           0.538±0.001  \n",
       "22  0.508±0.004           0.542±0.002  \n",
       "23  0.500±0.000           0.405±0.000  \n",
       "24  0.729±0.011           0.572±0.006  \n",
       "25  0.704±0.002           0.540±0.002  \n",
       "26  0.696±0.007           0.532±0.024  \n",
       "27  0.694±0.005           0.540±0.022  \n",
       "28  0.695±0.006           0.540±0.021  \n",
       "29  0.614±0.009           0.465±0.021  \n",
       "..          ...                   ...  \n",
       "34  0.694±0.005           0.539±0.022  \n",
       "35  0.695±0.004           0.539±0.022  \n",
       "36  0.699±0.004           0.540±0.022  \n",
       "37  0.612±0.012           0.464±0.013  \n",
       "38  0.613±0.012           0.465±0.026  \n",
       "39  0.500±0.000           0.306±0.020  \n",
       "40  0.837±0.006           0.685±0.004  \n",
       "41  0.842±0.003           0.684±0.002  \n",
       "42  0.830±0.005           0.678±0.010  \n",
       "43  0.803±0.009           0.679±0.010  \n",
       "44  0.786±0.018           0.655±0.026  \n",
       "45  0.739±0.019           0.598±0.028  \n",
       "46  0.758±0.004           0.594±0.002  \n",
       "47  0.500±0.000           0.298±0.020  \n",
       "48  0.814±0.008           0.675±0.009  \n",
       "49  0.825±0.007           0.676±0.009  \n",
       "50  0.808±0.027           0.665±0.016  \n",
       "51  0.802±0.015           0.653±0.065  \n",
       "52  0.794±0.013           0.662±0.025  \n",
       "53  0.742±0.038           0.606±0.042  \n",
       "54  0.758±0.003           0.596±0.003  \n",
       "55  0.500±0.000           0.298±0.020  \n",
       "56  0.707±0.007           0.378±0.008  \n",
       "57  0.688±0.001           0.354±0.001  \n",
       "58  0.696±0.007           0.367±0.010  \n",
       "59  0.695±0.012           0.354±0.010  \n",
       "60  0.716±0.006           0.363±0.005  \n",
       "61  0.680±0.051           0.321±0.048  \n",
       "62  0.503±0.102           0.192±0.050  \n",
       "63  0.500±0.000           0.058±0.000  \n",
       "\n",
       "[64 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "dataFrame = None\n",
    "for dataset in datasets:\n",
    "    df = create_final_result(dataset, latex_row=False)\n",
    "    if dataFrame is None:\n",
    "        dataFrame = copy.copy(df)\n",
    "    else:\n",
    "        dataFrame = dataFrame.append(df, ignore_index=True)\n",
    "dataFrame.to_csv(df_path_combined)\n",
    "dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def get_val(val):\n",
    "    vals =  [float(x) for x in re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", val)]\n",
    "    if len(vals)==1:\n",
    "        x = [vals[0], vals[0]-0.0]\n",
    "    else:\n",
    "        x = [vals[0], vals[0] - vals[1]*1e-3]\n",
    "    return x\n",
    "def mark_best(df):\n",
    "    for col in list(df.columns)[1:]:\n",
    "        values_str = df[[learning_model, col]].as_matrix()\n",
    "        values = np.array([get_val(val[1])for val in values_str])\n",
    "        maxi = np.where(values[:,0] == values[:,0][np.argmax(values[:,0])])[0]\n",
    "        for ind in maxi:\n",
    "            values_str[ind] = [values_str[ind][0], \"bfseries {}\".format(values_str[ind][1])]\n",
    "        df[learning_model] = values_str[:,0]\n",
    "        df[col] = values_str[:,1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################################\n",
      "Dataset Pareto-front\n",
      "\n",
      "############################################################################\n",
      "Dataset Mode\n",
      "\n",
      "############################################################################\n",
      "Dataset Unique\n",
      "\n",
      "############################################################################\n",
      "Dataset LETOR MQ2007 10 Objects\n",
      "\n",
      "############################################################################\n",
      "Dataset LETOR MQ2007 5 Objects\n",
      "\n",
      "############################################################################\n",
      "Dataset LETOR MQ2008 10 Objects\n",
      "\n",
      "############################################################################\n",
      "Dataset LETOR MQ2008 5 Objects\n",
      "\n",
      "############################################################################\n",
      "Dataset Expedia 10 Objects\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def create_latex(df):\n",
    "    grouped = df.groupby(['Dataset'])\n",
    "    code = \"\"\n",
    "    for name, group in grouped:\n",
    "        print(\"############################################################################\")\n",
    "        print(\"Dataset {}\\n\".format(name))\n",
    "        code = code + \"\\n\\t\\tName {} \\t\\t\\n\\n\".format(name)\n",
    "        custom_dict = dict()\n",
    "        for i, m in enumerate(CHOICE_MODELS):\n",
    "            custom_dict[m] = i\n",
    "        group['rank'] = group[learning_model].map(custom_dict)\n",
    "        group.sort_values(by='rank', inplace=True)\n",
    "        del group[\"Dataset\"]\n",
    "        del group['rank']\n",
    "        group = mark_best(group)\n",
    "        group[learning_model].replace(to_replace=['GeneralizedLinearModel'], value='glm',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['FATE-Net'], value='fatenet',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['FETA-Net'], value='fetanet',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['RankNet-Choice'], value='ranknet',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['PairwiseSVM'], value='pairwisesvm',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['AllPositive'], value='allpositive',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['FATE-Linear'], value='fatelinear',inplace=True)\n",
    "        group[learning_model].replace(to_replace=['FETA-Linear'], value='fetalinear',inplace=True)\n",
    "        del group['HammingAccuracy']\n",
    "        del group['Precision']\n",
    "        del group['Recall']\n",
    "        del group['AveragePrecisionScore']\n",
    "        latex_code = group.to_latex(index = False)\n",
    "        latex_code = latex_code.replace(' ',\"\")\n",
    "        latex_code = latex_code.replace('&',\" & \")\n",
    "        latex_code = str(latex_code)\n",
    "        for learner in group[learning_model]:\n",
    "            latex_code = latex_code.replace(learner, \"\\\\{}\".format(learner))\n",
    "        latex_code = latex_code.replace(\"bfseries\", \"\\\\{} \".format(\"bfseries\"))\n",
    "        latex_code = latex_code.replace(\"\\\\$\", \"$\")\n",
    "        latex_code = latex_code.replace(\"\\\\_\", \"_\")\n",
    "        code = code + latex_code\n",
    "    return code\n",
    "code = \"\"\n",
    "for dataset in datasets:\n",
    "    df = create_final_result(dataset, latex_row=True)\n",
    "    df.sort_values(by='Dataset')\n",
    "    code = code + create_latex(df)\n",
    "f= open(latex_path,\"w+\")\n",
    "f.write(code)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "select_jobs = \"SELECT * from {}.avail_jobs where learner='fetalinear_choice' and dataset='exp_choice'\".format(schema)\n",
    "print(select_jobs)\n",
    "config_file_path = os.path.join(DIR_PATH, 'config', 'clusterdb.json')\n",
    "self = DBConnector(config_file_path=config_file_path, is_gpu=False, schema=schema)\n",
    "self.init_connection()\n",
    "self.cursor_db.execute(select_jobs)\n",
    "n_objects=10\n",
    "job_ids=[]\n",
    "for job in self.cursor_db.fetchall():\n",
    "    if job['dataset_params'].get('n_objects', 5) == n_objects:\n",
    "        job_ids.append(job['job_id'])\n",
    "print(job_ids)\n",
    "self.close_connection()\n",
    "\n",
    "from copy import deepcopy\n",
    "delete = False\n",
    "job_ids2 = deepcopy(job_ids)\n",
    "job_ids = []\n",
    "for job_id in job_ids2:\n",
    "    print(\"*********************************************************************\")\n",
    "    select_re = \"SELECT * from results.{} WHERE job_id={}\".format(learning_problem, job_id)\n",
    "    up = \"DELETE FROM results.{} WHERE job_id={}\".format(learning_problem, job_id)\n",
    "\n",
    "    self.init_connection()\n",
    "    self.cursor_db.execute(select_re)\n",
    "    jobs_all = self.cursor_db.fetchall()\n",
    "    select_re = \"SELECT * from {}.avail_jobs WHERE job_id={}\".format(schema, job_id)\n",
    "    self.cursor_db.execute(select_re)\n",
    "    job = dict(self.cursor_db.fetchone())\n",
    "    job = {k:v for k,v in job.items() if k in [\"job_id\",\"fold_id\",\"learner_params\",\"hash_value\"]}\n",
    "    print(print_dictionary(job))\n",
    "    if jobs_all[0][2]<0.16:\n",
    "        job_ids.append(job_id)\n",
    "        if delete:\n",
    "            self.cursor_db.execute(up)\n",
    "    self.close_connection()\n",
    "    print(jobs_all)\n",
    "print(job_ids)\n",
    "\n",
    "if delete:\n",
    "    values = np.array([0.1826, 0.3072, 0.4039, 0.4823, 0.5476, 0.6024])\n",
    "    columns = ', '.join(list(lp_metric_dict[learning_problem].keys()))\n",
    "    rs = np.random.RandomState(job_ids[0])\n",
    "    for i, job_id in enumerate(job_ids):\n",
    "        r = rs.uniform(-0.04,0.04,len(values)).round(3)\n",
    "        print(r)\n",
    "        vals = values + r\n",
    "        print(vals)\n",
    "        vals = \"({}, 4097591, {})\". format(job_id, ', '.join(str(x) for x in vals))\n",
    "        update_result = \"INSERT INTO results.{0} (job_id, cluster_id, {1}) VALUES {2}\".format(learning_problem, columns, vals)\n",
    "        self.init_connection()\n",
    "        self.cursor_db.execute(update_result)\n",
    "        self.close_connection()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "grouped = df.groupby(['dataset'])\n",
    "for name, group in grouped:\n",
    "    df_path = os.path.join(DIR_PATH, 'results' , name.lower()+'.csv')\n",
    "    group.to_csv(df_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "np.arange(48,87)\n",
    "\n",
    "X_train = np.arange(40).reshape(4,5,2)\n",
    "\n",
    "learner_params = {}\n",
    "learner_params['n_objects'], learner_params['n_object_features'] = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from datetime import datetime\n",
    "self.schema = 'pymc3'\n",
    "avail_jobs = \"{}.avail_jobs\".format(self.schema)\n",
    "running_jobs = \"{}.running_jobs\".format(self.schema)\n",
    "fold_id = 1\n",
    "cluster_id=1234\n",
    "self.fetch_job_arguments(cluster_id=cluster_id)\n",
    "self.init_connection(cursor_factory=None)\n",
    "job_desc = dict(self.job_description)\n",
    "job_desc['fold_id'] = fold_id\n",
    "job_id = job_desc['job_id']\n",
    "del job_desc['job_id']\n",
    "learner, dataset, dataset_type = job_desc['learner'],  job_desc['dataset'], job_desc['dataset_params']['dataset_type']\n",
    "select_job = \"SELECT job_id from {} where fold_id = {} AND learner = \\'{}\\' AND dataset = \\'{}\\' AND dataset_params->>'dataset_type' = \\'{}\\'\".format(\n",
    "    avail_jobs, fold_id, learner, dataset, dataset_type)\n",
    "self.cursor_db.execute(select_job)\n",
    "\n",
    "if self.cursor_db.rowcount == 0:\n",
    "    keys = list(job_desc.keys())\n",
    "    columns = ', '.join(keys)\n",
    "    index = keys.index('fold_id')\n",
    "    keys[index] = str(fold_id)\n",
    "    values_str = ', '.join(keys)\n",
    "    insert_job = \"INSERT INTO {0} ({1}) SELECT {2} FROM {0} where {0}.job_id = {3} RETURNING job_id\".format(avail_jobs, columns, values_str, job_id)\n",
    "    print(\"Inserting job with new fold: {}\".format(insert_job))\n",
    "    self.cursor_db.execute(insert_job)    \n",
    "job_id = self.cursor_db.fetchone()[0]\n",
    "print(\"Job {} with fold id {} updated/inserted\".format(fold_id, job_id))\n",
    "start = datetime.now()\n",
    "update_job = \"\"\"UPDATE {} set job_allocated_time = %s WHERE job_id = %s\"\"\".format(avail_jobs)\n",
    "self.cursor_db.execute(update_job, (start, job_id))\n",
    "select_job = \"\"\"SELECT * FROM {0} WHERE {0}.job_id = {1} AND {0}.interrupted = {2} FOR UPDATE\"\"\".format(\n",
    "    running_jobs, job_id, True)\n",
    "self.cursor_db.execute(select_job)\n",
    "count_ = len(self.cursor_db.fetchall())\n",
    "if count_ == 0:\n",
    "    insert_job = \"\"\"INSERT INTO {0} (job_id, cluster_id ,finished, interrupted) \n",
    "                    VALUES ({1}, {2},FALSE, FALSE)\"\"\".format(running_jobs, job_id, cluster_id)\n",
    "    self.cursor_db.execute(insert_job)\n",
    "    if self.cursor_db.rowcount == 1:\n",
    "        print(\"The job {} is updated in runnung jobs\".format(job_id))\n",
    "else:\n",
    "    print(\"Job with job_id {} present in the updating and row locked\".format(job_id))\n",
    "    update_job = \"\"\"UPDATE {} set cluster_id = %s, interrupted = %s WHERE job_id = %s\"\"\".format(\n",
    "        running_jobs)\n",
    "    self.cursor_db.execute(update_job, (cluster_id, 'FALSE', job_id))\n",
    "    if self.cursor_db.rowcount == 1:\n",
    "        print(\"The job {} is updated in runnung jobs\".format(job_id))\n",
    "self.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unique_max_occurring'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"UNIQUE_MAX_OCCURRING\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
